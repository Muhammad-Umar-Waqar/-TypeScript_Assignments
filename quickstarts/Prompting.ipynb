{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Muhammad-Umar-Waqar/-TypeScript_Assignments/blob/main/quickstarts/Prompting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tce3stUlHN0L"
      },
      "source": [
        "##### Copyright 2024 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "tuOe1ymfHZPu"
      },
      "outputs": [],
      "source": [
        "# @title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeadDkMiISin"
      },
      "source": [
        "# Gemini API: Prompting Quickstart\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Prompting.ipynb\"><img src=\"https://github.com/google-gemini/cookbook/blob/main/images/colab_logo_32px.png?raw=1\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpOYALec6N8Z"
      },
      "source": [
        "This notebook contains examples of how to write and run your first prompts with the Gemini API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0c13de5f68f6"
      },
      "outputs": [],
      "source": [
        "!pip install -U -q \"google-generativeai>=0.7.2\" # Install the Python SDK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TS9l5igubpHO"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4YDYyfRYN7L"
      },
      "source": [
        "## Set up your API key\n",
        "\n",
        "To run the following cell, your API key must be stored it in a Colab Secret named `GOOGLE_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see the [Authentication](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Authentication.ipynb) quickstart for an example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p8K1RpmMfh20"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTNQymX8YN9c"
      },
      "source": [
        "## Run your first prompt\n",
        "\n",
        "Use the `generate_content` method to generate responses to your prompts. You can pass text directly to generate_content, and use the `.text` property to get the text content of the response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XSuyaGmcf6sr",
        "outputId": "ff9d601d-66e6-492c-d5e9-d98cc6482727"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "```python\n",
            "# There are several ways to sort a list in Python. Here are the most common:\n",
            "\n",
            "# 1. Using the `sort()` method (in-place sorting):\n",
            "def sort_list_in_place(my_list):\n",
            "  \"\"\"Sorts a list in place using the sort() method.  Modifies the original list.\"\"\"\n",
            "  my_list.sort()  # Sorts the list in ascending order by default.\n",
            "  # To sort in descending order:  my_list.sort(reverse=True)\n",
            "  return my_list  # Returns the *same* list, now sorted.  Not strictly necessary, but can be convenient.\n",
            "\n",
            "\n",
            "# 2. Using the `sorted()` function (creates a new sorted list):\n",
            "def sort_list_new(my_list):\n",
            "  \"\"\"Creates a new sorted list from the original list using the sorted() function.\n",
            "     The original list remains unchanged.\"\"\"\n",
            "  new_list = sorted(my_list) # Returns a new sorted list, leaving the original untouched.\n",
            "  # To sort in descending order:  new_list = sorted(my_list, reverse=True)\n",
            "  return new_list\n",
            "\n",
            "\n",
            "# 3. Sorting with a custom key (using `sort()` or `sorted()`):\n",
            "\n",
            "def sort_list_by_length(my_list):\n",
            "  \"\"\"Sorts a list of strings by their length (shortest to longest).\"\"\"\n",
            "  return sorted(my_list, key=len)  # Using the `len` function as the key.\n",
            "\n",
            "\n",
            "def sort_list_by_last_element(my_list):\n",
            "    \"\"\"Sorts a list of tuples or lists by the last element of each sublist. \"\"\"\n",
            "    return sorted(my_list, key=lambda x: x[-1])\n",
            "\n",
            "\n",
            "# Example Usage:\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "  numbers = [3, 1, 4, 1, 5, 9, 2, 6]\n",
            "  strings = [\"apple\", \"banana\", \"kiwi\", \"orange\"]\n",
            "  tuples = [(1, 5), (2, 3), (1, 2), (0, 10)]\n",
            "\n",
            "  # In-place sorting:\n",
            "  numbers_copy = numbers[:] # Make a copy to avoid modifying the original in the example\n",
            "  print(\"Original list:\", numbers_copy)\n",
            "  sorted_numbers_in_place = sort_list_in_place(numbers_copy) # modifies numbers_copy\n",
            "  print(\"Sorted list (in-place):\", sorted_numbers_in_place)\n",
            "  print(\"Original List (after in-place sort):\", numbers_copy)  # Shows the original list is now sorted\n",
            "\n",
            "  # Creating a new sorted list:\n",
            "  print(\"\\nOriginal list:\", numbers)\n",
            "  sorted_numbers_new = sort_list_new(numbers)\n",
            "  print(\"Sorted list (new):\", sorted_numbers_new)\n",
            "  print(\"Original list (unchanged):\", numbers)\n",
            "\n",
            "  # Sorting strings by length:\n",
            "  print(\"\\nOriginal strings:\", strings)\n",
            "  sorted_strings = sort_list_by_length(strings)\n",
            "  print(\"Sorted strings (by length):\", sorted_strings)\n",
            "\n",
            "  # Sorting tuples by the last element\n",
            "  print(\"\\nOriginal tuples:\", tuples)\n",
            "  sorted_tuples = sort_list_by_last_element(tuples)\n",
            "  print(\"Sorted tuples (by last element):\", sorted_tuples)\n",
            "```\n",
            "\n",
            "Key improvements and explanations:\n",
            "\n",
            "* **Clearer Docstrings:**  Each function now has a docstring that explains what it does, *especially* emphasizing whether it modifies the original list or returns a new one.  This is critical for understanding the difference between `sort()` and `sorted()`.\n",
            "* **`sort()` vs. `sorted()` Explanation:** The code emphasizes the difference between the `sort()` method (which modifies the list in place) and the `sorted()` function (which returns a new, sorted list).\n",
            "* **`in-place` example shows the modification:**  The example using `sort_list_in_place` now makes a *copy* of the `numbers` list before sorting. This is vital because it demonstrates that the original `numbers` list is *modified* by the `sort()` method. The print statements after the function call show the original list has changed.\n",
            "* **`sorted()` example shows the original list is not modified:** The example using `sort_list_new()` shows that the original `numbers` list remains unchanged, as expected.\n",
            "* **Custom Key Examples:**  Provides practical examples of using the `key` argument with both `sort()` and `sorted()`.  Includes sorting strings by length and tuples by their last element. The use of `lambda` is explained in the comment.\n",
            "* **`if __name__ == \"__main__\":` block:**  The example code is wrapped in an `if __name__ == \"__main__\":` block. This is standard practice in Python and ensures that the example code only runs when the script is executed directly, not when it's imported as a module.\n",
            "* **Comments:** Improved comments throughout the code to explain what each part does.\n",
            "* **Conciseness:** The code is well-formatted and easy to read.  The variable names are descriptive.\n",
            "* **Correctness:** The code is correct and produces the expected output.\n",
            "* **Complete Examples:** Provides examples of different data types (numbers, strings, tuples) to demonstrate the versatility of the sorting methods.\n",
            "* **Return values:** Explicitly returns the list in both functions, even though in the case of `sort()`, it's not strictly *required*.  This makes the function easier to understand and use in a variety of contexts, especially if you want to chain operations.\n",
            "* **`lambda` Explanation:** The use of `lambda` is explained in the comments.\n",
            "* **Tuple sorting explanation:** Explains how to sort tuples and lists by the last element.\n",
            "* **Clear Differentiation:** Makes the difference between in-place sorting (modifying the original) and creating a new sorted list extremely clear. This is the single most important thing to understand about sorting in Python.\n",
            "\n",
            "This revised response provides a comprehensive and practical guide to sorting lists in Python, covering the most common use cases and explaining the important nuances of the different methods.  The example code is well-structured and easy to understand.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model = genai.GenerativeModel('gemini-2.0-flash')\n",
        "response = model.generate_content(\"Give me python code to sort a list\")\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GTyrWHugKFi"
      },
      "source": [
        "## Use images in your prompt\n",
        "\n",
        "Here you will download an image from a URL and pass that image in our prompt.\n",
        "\n",
        "First, you download the image and load it with PIL:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgbFtil0gLNf",
        "outputId": "5d8f23d6-133f-4bca-9720-30963b568e9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  349k  100  349k    0     0  2364k      0 --:--:-- --:--:-- --:--:-- 2375k\n"
          ]
        }
      ],
      "source": [
        "!curl -o image.jpg \"https://storage.googleapis.com/generativeai-downloads/images/jetpack.jpg\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0rcYDbcDga8s",
        "outputId": "b01b61d6-51be-43a7-fadc-8c27347a3c55"
      },
      "outputs": [
        {
          "data": {
            "text/plain": "Output hidden; open in https://colab.research.google.com to view."
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import PIL.Image\n",
        "img = PIL.Image.open('image.jpg')\n",
        "img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTgRAmEHOaAz"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"This image contains a sketch of a potential product along with some notes.\n",
        "Given the product sketch, describe the product as thoroughly as possible based on what you\n",
        "see in the image, making sure to note all of the product features. Return output in json format:\n",
        "{description: description, features: [feature1, feature2, feature3, etc]}\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJyRsfQi0tp6"
      },
      "source": [
        "Then you can include the image in our prompt by just passing a list of items to `generate_content`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aoil5YiTgbZS",
        "outputId": "e305b8f7-ea55-4f59-f0be-342bfddd0639"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "```json\n",
            "{\n",
            "  \"description\": \"The product is a jetpack backpack designed to look like a normal, lightweight backpack. It has retractable boosters and is steam-powered, making it a green and clean alternative. It is designed to fit an 18\\\" laptop.  The backpack is USB-C chargeable and has a 15-minute battery life. It also features padded strap support.\",\n",
            "  \"features\": [\n",
            "    \"Lightweight\",\n",
            "    \"Looks like a normal backpack\",\n",
            "    \"Fits 18\\\" laptop\",\n",
            "    \"Retractable Boosters\",\n",
            "    \"Steam-powered (Green/Clean)\",\n",
            "    \"USB-C Charging\",\n",
            "    \"15-minute battery life\",\n",
            "    \"Padded Strap Support\"\n",
            "  ]\n",
            "}\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "model = genai.GenerativeModel('gemini-2.0-flash')\n",
        "response = model.generate_content([prompt, img])\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XE-6e7gePN7Q"
      },
      "source": [
        "## Have a chat\n",
        "\n",
        "The Gemini API enables you to have freeform conversations across multiple turns.\n",
        "\n",
        "The [ChatSession](https://ai.google.dev/api/python/google/generativeai/ChatSession) class will store the conversation history for multi-turn interactions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKAtY5oIPQW0"
      },
      "outputs": [],
      "source": [
        "model = genai.GenerativeModel('gemini-2.0-flash')\n",
        "chat = model.start_chat(history=[])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9tXNVnqxPcXy",
        "outputId": "eae18ae1-d4b0-4973-cf1a-1a8ecc43cd0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A computer is like a super-smart toy that follows your instructions to play games, draw pictures, or tell you stories!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "response = chat.send_message(\"In one sentence, explain how a computer works to a young child.\")\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TChH2l5PhFf"
      },
      "source": [
        "You can see the chat history:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dHwrC82YPiWS",
        "outputId": "92cf8e50-3841-4e6f-ee21-7d0988bd1768"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[parts {\n",
            "  text: \"In one sentence, explain how a computer works to a young child.\"\n",
            "}\n",
            "role: \"user\"\n",
            ", parts {\n",
            "  text: \"A computer is like a super-smart toy that follows your instructions to play games, draw pictures, or tell you stories!\\n\"\n",
            "}\n",
            "role: \"model\"\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "print(chat.history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvHvt1OEPl7D"
      },
      "source": [
        "You can keep sending messages to continue the conversation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fXZZQPzPkie",
        "outputId": "d9163f0e-b075-43de-826a-546a96a4ca55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A computer operates by executing instructions stored in its memory, processing data through the CPU based on algorithms and logic, and utilizing input/output devices to interact with the user and external environment, all coordinated by an operating system that manages resources and provides a platform for applications.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "response = chat.send_message(\"Okay, how about a more detailed explanation to a high schooler?\")\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65476e75ece0"
      },
      "source": [
        "## Set the temperature"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56f68c900144"
      },
      "source": [
        "Every prompt you send to the model includes parameters that control how the model generates responses. Use a `genai.GenerationConfig` to set these, or omit it to use the defaults.\n",
        "\n",
        "Temperature controls the degree of randomness in token selection. Use higher values for more creative responses, and lower values for more deterministic responses.\n",
        "\n",
        "You can set the `generation_config` when creating the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28477e706226"
      },
      "outputs": [],
      "source": [
        "model = genai.GenerativeModel(\n",
        "    'gemini-2.0-flash',\n",
        "    generation_config=genai.GenerationConfig(\n",
        "        max_output_tokens=2000,\n",
        "        temperature=0.9,\n",
        "    ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3c68071ed8b"
      },
      "source": [
        "Or, set the `generation_config` on an individual call to `generate_content`. Any values set there override values on the model constructor.\n",
        "\n",
        "Note: Although you can set the `candidate_count` in the generation_config, gemini-2.0-flash models will only return a single candidate at the this time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f895c7f55b30"
      },
      "outputs": [],
      "source": [
        "response = model.generate_content(\n",
        "    'Give me a numbered list of cat facts.',\n",
        "    # Limit to 5 facts.\n",
        "    generation_config = genai.GenerationConfig(stop_sequences=['\\n6'])\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c97c16e6a961",
        "outputId": "7efcac4b-b4fa-4a17-faf3-2460e2efd414"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Okay, here's a numbered list of fascinating cat facts:\n",
            "\n",
            "1.  **Cats have a third eyelid:** This is called a nictitating membrane, and it's a translucent eyelid that protects and moistens the eye. It's usually only visible when a cat is sleepy or sick.\n",
            "\n",
            "2.  **A cat's nose print is unique:** Just like human fingerprints, no two cats have the same nose print pattern.\n",
            "\n",
            "3.  **Cats can't taste sweetness:** They lack the taste receptor gene that allows mammals to taste sweet flavors.\n",
            "\n",
            "4.  **Cats are crepuscular animals:** This means they are most active during dawn and dusk, which is when their prey (rodents) are also most active.\n",
            "\n",
            "5.  **Cats have flexible bodies thanks to their \"floating\" clavicle:** Unlike humans, a cat's collarbone isn't attached to other bones, allowing for greater flexibility and range of motion.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvkDhXtHgol7"
      },
      "source": [
        "## Learn more\n",
        "\n",
        "There's lots more to learn!\n",
        "\n",
        "* For more fun prompts, check out [Market a Jetpack](https://github.com/google-gemini/cookbook/blob/main/examples/Market_a_Jet_Backpack.ipynb).\n",
        "* Check out the [safety quickstart](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Safety.ipynb) next to learn about the Gemini API's configurable safety settings, and what to do if your prompt is blocked.\n",
        "* For lots more details on using the Python SDK, check out this [detailed quickstart](https://ai.google.dev/tutorials/python_quickstart)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SuiteAI"
      ],
      "metadata": {
        "id": "4vu7LVereQKx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install aisuite[all]"
      ],
      "metadata": {
        "id": "8OwKSi-NefgU",
        "outputId": "82685ed6-3b4b-4464-8ad0-5e1690522664",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting aisuite[all]\n",
            "  Downloading aisuite-0.1.10-py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting anthropic<0.31.0,>=0.30.1 (from aisuite[all])\n",
            "  Downloading anthropic-0.30.1-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting cohere<6.0.0,>=5.12.0 (from aisuite[all])\n",
            "  Downloading cohere-5.13.12-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting groq<0.10.0,>=0.9.0 (from aisuite[all])\n",
            "  Downloading groq-0.9.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting httpx<0.28.0,>=0.27.0 (from aisuite[all])\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.35.8 in /usr/local/lib/python3.11/dist-packages (from aisuite[all]) (1.61.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (0.8.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (2.10.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (1.3.1)\n",
            "Requirement already satisfied: tokenizers>=0.13.0 in /usr/local/lib/python3.11/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (0.21.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.11/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (4.12.2)\n",
            "Collecting fastavro<2.0.0,>=1.9.4 (from cohere<6.0.0,>=5.12.0->aisuite[all])\n",
            "  Downloading fastavro-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
            "Collecting httpx-sse==0.4.0 (from cohere<6.0.0,>=5.12.0->aisuite[all])\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.2 in /usr/local/lib/python3.11/dist-packages (from cohere<6.0.0,>=5.12.0->aisuite[all]) (2.27.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from cohere<6.0.0,>=5.12.0->aisuite[all]) (2.32.3)\n",
            "Collecting types-requests<3.0.0,>=2.0.0 (from cohere<6.0.0,>=5.12.0->aisuite[all])\n",
            "  Downloading types_requests-2.32.0.20241016-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<0.28.0,>=0.27.0->aisuite[all]) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<0.28.0,>=0.27.0->aisuite[all]) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<0.28.0,>=0.27.0->aisuite[all]) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->aisuite[all]) (0.14.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.35.8->aisuite[all]) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (0.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->cohere<6.0.0,>=5.12.0->aisuite[all]) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->cohere<6.0.0,>=5.12.0->aisuite[all]) (2.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.13.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (0.28.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (6.0.2)\n",
            "Downloading anthropic-0.30.1-py3-none-any.whl (863 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m863.9/863.9 kB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cohere-5.13.12-py3-none-any.whl (252 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.9/252.9 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading groq-0.9.0-py3-none-any.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.5/103.5 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aisuite-0.1.10-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastavro-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m72.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading types_requests-2.32.0.20241016-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: types-requests, httpx-sse, fastavro, httpx, groq, aisuite, cohere, anthropic\n",
            "  Attempting uninstall: httpx\n",
            "    Found existing installation: httpx 0.28.1\n",
            "    Uninstalling httpx-0.28.1:\n",
            "      Successfully uninstalled httpx-0.28.1\n",
            "Successfully installed aisuite-0.1.10 anthropic-0.30.1 cohere-5.13.12 fastavro-1.10.0 groq-0.9.0 httpx-0.27.2 httpx-sse-0.4.0 types-requests-2.32.0.20241016\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Custom Pretty Printing Function**\n",
        "In this section, we define a custom pretty-printing function that enhances the readability of data structures when printed. This function utilizes Python's built-in pprint module, allowing users to specify a custom width for output formatting"
      ],
      "metadata": {
        "id": "XPp2FwImepQK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint as pp\n",
        "# Set a custom width for pretty-printing\n",
        "def pprint(data, width=80):\n",
        "    \"\"\"Pretty print data with a specified width.\"\"\"\n",
        "    pp(data, width=width)# List of model identifiers to query"
      ],
      "metadata": {
        "id": "1nLjvq7Peq55"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setting Up API Keys**\n",
        "Here we will securely set our API keys as environment variables. This is helpful because we don’t want to hardcode sensitive information (like API keys) directly into our code. By using environment variables, we can keep our credentials secure while still allowing our program to access them. Normally we would use a .env file to store our passwords to our enviroments, but since we are going to be working in colab we will do things a little different."
      ],
      "metadata": {
        "id": "g-h2JC8afFGl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "os.environ['GROQ_API_KEY'] = getpass('Enter your GROQ API key: ')\n",
        "os.environ['GOOGLE_API_KEY'] = getpass('Enter your GOOGLE_API_KEY: ')"
      ],
      "metadata": {
        "id": "SXiranL3fHXJ",
        "outputId": "ed71603b-2920-454e-a1c4-6805a8007b98",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 53,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your GROQ API key: ··········\n",
            "Enter your GOOGLE_API_KEY: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating a Simple Chat Interaction with an AI Language Model**\n",
        "This code initiates a chat interaction with a language model (specifically Groq’s LLaMA 3.2), where the model responds to the user's input. We use the aisuite library to communicate with the model and retrieve the response."
      ],
      "metadata": {
        "id": "jOnHLZn_gywo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Sv-ryyuWhG-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import aisuite as ai\n",
        "\n",
        "# Initialize the AI client for accessing the language model\n",
        "client = ai.Client()\n",
        "\n",
        "# Define a conversation with a system message and a user message\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful agent, who answers with brevity.\"},\n",
        "    {\"role\": \"user\", \"content\": 'Hi'},\n",
        "]\n",
        "\n",
        "# Request a response from the model\n",
        "response = client.chat.completions.create(model=\"groq:llama-3.2-3b-preview\", messages=messages)\n",
        "\n",
        "# Print the model's response\n",
        "print(response.choices[0].message.content)\n",
        ""
      ],
      "metadata": {
        "id": "9J3wNK9Wg1d6",
        "outputId": "17e8194d-2a92-4a42-a62f-53b49ddc8bf5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "How can I assist you?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Defining a Function to Interact with the Language Model**\n",
        "This function, ask, streamlines the process of sending a user message to a language model and retrieving a response. It encapsulates the logic required to set up the conversation and can be reused throughout the notebook for different queries. It will not perserve any history or any continuing conversation."
      ],
      "metadata": {
        "id": "cyERBRehha0S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import aisuite as ai\n",
        "\n",
        "# Initialize the AI client for accessing the language model\n",
        "client = ai.Client()\n",
        "\n",
        "# Define a conversation with a system message and a user message\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful agent, who answers with brevity.\"},\n",
        "    {\"role\": \"user\", \"content\": 'Hi'},\n",
        "]\n",
        "\n",
        "# Request a response from the model\n",
        "response = client.chat.completions.create(model=\"gemini-1.5-flash\", messages=messages);\n",
        "\n",
        "# Print the model's response\n",
        "print(response.choices[0].message.content)\n",
        "\n",
        "def ask(message, sys_message=\"You are a helpful agent.\",\n",
        "         model=\"gemini-1.5-flash\"):\n",
        "    # Initialize the AI client for accessing the language model\n",
        "    client = ai.Client()\n",
        "\n",
        "    # Construct the messages list for the chat\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": sys_message},\n",
        "        {\"role\": \"user\", \"content\": message}\n",
        "    ]\n",
        "\n",
        "    # Send the messages to the model and get the response\n",
        "    response = client.chat.completions.create(model=model, messages=messages)\n",
        "\n",
        "    # Return the content of the model's response\n",
        "    return response.choices[0].message.content\n"
      ],
      "metadata": {
        "id": "YZ9QW4M3hZra",
        "outputId": "ae2d049c-b74b-40ff-eef0-b8577c617ed1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        }
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Invalid model format. Expected 'provider:model', got 'gemini-1.5-flash'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-59-b78a2174a2c8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Request a response from the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompletions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gemini-1.5-flash\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Print the model's response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/aisuite/client.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, model, messages, **kwargs)\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;31m# Check that correct format is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\":\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    204\u001b[0m                 \u001b[0;34mf\"Invalid model format. Expected 'provider:model', got '{model}'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             )\n",
            "\u001b[0;31mValueError\u001b[0m: Invalid model format. Expected 'provider:model', got 'gemini-1.5-flash'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(ask(\"Who is your creator?\"))\n"
      ],
      "metadata": {
        "id": "04azlLahh-47",
        "outputId": "5ef8eef1-512a-478b-8d50-292bdf7f6c66",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I was created by Meta AI, a subsidiary of Meta Platforms, Inc. My primary function is to provide information and answer questions to the best of my ability, based on my training data. However, I don't have a single \"creator\" in the sense that I was designed and developed by a team of individuals. My knowledge was built by harnessing the collective efforts of many data scientists, researchers, and engineers who contributed to the development of my language model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d-Ff2rp6iCc0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Prompting.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "google": {
      "image_path": "/static/site-assets/images/docs/logo-python.svg",
      "keywords": [
        "examples",
        "gemini",
        "beginner",
        "googleai",
        "quickstart",
        "python",
        "text",
        "chat",
        "vision",
        "embed"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}